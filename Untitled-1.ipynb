{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seung\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 지정된 프로시저를 찾을 수 없습니다'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./aclImdb_v1/aclImdb/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "for folder_name in os.listdir(\"./aclImdb_v1/aclImdb/train\"):\n",
    "    folder_path = os.path.join(\"./aclImdb_v1/aclImdb/train\",folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                data_train.append({'Folder': folder_name, 'File': file_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.DataFrame(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'pos', 'unsup'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['Folder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>0_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>10001_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>10002_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>10003_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>unsup</td>\n",
       "      <td>9998_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>unsup</td>\n",
       "      <td>9999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>unsup</td>\n",
       "      <td>999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>unsup</td>\n",
       "      <td>99_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>unsup</td>\n",
       "      <td>9_0.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Folder         File\n",
       "0        neg      0_3.txt\n",
       "1        neg  10000_4.txt\n",
       "2        neg  10001_4.txt\n",
       "3        neg  10002_1.txt\n",
       "4        neg  10003_1.txt\n",
       "...      ...          ...\n",
       "74995  unsup   9998_0.txt\n",
       "74996  unsup   9999_0.txt\n",
       "74997  unsup    999_0.txt\n",
       "74998  unsup     99_0.txt\n",
       "74999  unsup      9_0.txt\n",
       "\n",
       "[75000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = data_train[(data_train['Folder'] == \"neg\") | (data_train['Folder'] == \"pos\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, train = train_test_split(train_dset, test_size = 0.8, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_file, dataset_path, tokenizer, blocksize):\n",
    "        self.data = dataset_file\n",
    "        self.path = dataset_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.blocksize = blocksize\n",
    "        self.processed_data = []\n",
    "        self.dataEDA()\n",
    "\n",
    "    \n",
    "    def dataEDA(self):\n",
    "        for idx in range(len(self.data)):\n",
    "            folder, file = train_dset.iloc[idx]\n",
    "            target_path = os.path.join(self.path,folder,file)\n",
    "            with open(target_path,'r',encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                token_text = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "                for i in range(0, len(token_text) - self.blocksize + 1, self.blocksize):\n",
    "                    self.processed_data.append(self.tokenizer.build_inputs_with_special_tokens(token_text[i:i + self.blocksize]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return torch.tensor(self.processed_data[idx])        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seung\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] : Epoch 0 loss :4.547640323638916\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     32\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mvalid_loader\u001b[49m):\n\u001b[0;32m     34\u001b[0m         inputs,labels \u001b[38;5;241m=\u001b[39m (data,data)\n\u001b[0;32m     35\u001b[0m         inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_loader' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "model_name = 'heegyu/gpt2-emotion'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "train_set = CustomDataset(train, train_path,tokenizer, 128)\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "val_set = CustomDataset(val, train_path, tokenizer, 128)\n",
    "valid_loader = DataLoader(val_set,batch_size =4, shuffle=True)\n",
    "\n",
    "param = model.parameters()\n",
    "optimizer = torch.optim.AdamW(param, lr=learning_rate, eps=adam_epsilon)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "best_valid = 1000\n",
    "for step in range(epochs):  \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs = data.to(device)\n",
    "        labels = inputs.clone() \n",
    "        model.zero_grad()\n",
    "        output = model(inputs,labels = labels)\n",
    "        loss=output.loss\n",
    "        logits = output.logits\n",
    "        \n",
    "        soft_labels = F.softmax(logits, dim=-1)\n",
    "        log_soft_labels = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        kl_loss = F.kl_div(log_soft_labels, soft_labels, reduction='batchmean')\n",
    "        combined_loss = loss + kl_loss\n",
    "        combined_loss.backward()\n",
    "        train_loss = combined_loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        print(f'[Train] : Epoch {step} Step {i} loss :{train_loss}')\n",
    "    \n",
    "    if step%2==0 or step==epochs-1:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss=0.0\n",
    "            for i, data in enumerate(valid_loader):\n",
    "                inputs = data.to(device)\n",
    "                labels = inputs.clone()\n",
    "                output= model(inputs,labels)\n",
    "\n",
    "                loss = output.loss\n",
    "                soft_labels = F.softmax(logits, dim=-1)\n",
    "                log_soft_labels = F.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                kl_loss = F.kl_div(log_soft_labels, soft_labels, reduction='batchmean')\n",
    "\n",
    "                combined_loss = loss + kl_loss\n",
    "                valid_loss += combined_loss.item()\n",
    "            temp_loss = valid_loss/len(valid_loader)\n",
    "            print(f'### [Valid] Epoch {step} loss : {temp_loss} ###')\n",
    "            \n",
    "    if best_valid > temp_loss:\n",
    "        best_valid = temp_loss\n",
    "        ES_count = 0\n",
    "\n",
    "    else:\n",
    "        ES_count+=1\n",
    "    \n",
    "    if ES_count==3:\n",
    "        torch.save(model.state_dict(), \"./gpt_trained.pt\")\n",
    "        break   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 50257])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
